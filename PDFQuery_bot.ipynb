{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio google-generativeai chromadb langchain-community pypdf\n",
        "!npm install -g localtunnel\n",
        "\n",
        "code=\"\"\"\n",
        "\n",
        "import os\n",
        "import gradio as gr\n",
        "import google.generativeai as genai\n",
        "import chromadb\n",
        "from chromadb.utils.embedding_functions import EmbeddingFunction\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from google.api_core import retry\n",
        "\n",
        "# Set Gemini API key\n",
        "GEMINI_API_KEY = os.getenv(\"GENERATIVE_AI_API_KEY\")\n",
        "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "\n",
        "# Gemini Embedding Function for Chroma\n",
        "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
        "    def __init__(self, document_mode=True):\n",
        "        self.document_mode = document_mode\n",
        "\n",
        "    def __call__(self, input):\n",
        "        task_type = \"retrieval_document\" if self.document_mode else \"retrieval_query\"\n",
        "        response = genai.embed_content(\n",
        "            model=\"models/text-embedding-004\",\n",
        "            content=input,\n",
        "            task_type=task_type,\n",
        "            request_options={\"retry\": retry.Retry(predicate=retry.if_transient_error)},\n",
        "        )\n",
        "        return response[\"embedding\"]\n",
        "\n",
        "# Load and chunk PDF\n",
        "def process_pdf(file_path):\n",
        "    loader = PyPDFLoader(file_path)\n",
        "    pages = loader.load()\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "    chunks = [doc.page_content for doc in splitter.split_documents(pages)]\n",
        "    return chunks\n",
        "\n",
        "# Store chunks into Chroma\n",
        "def load_into_chroma(chunks):\n",
        "    embed_fn = GeminiEmbeddingFunction(document_mode=True)\n",
        "    chroma_client = chromadb.Client()\n",
        "    collection = chroma_client.get_or_create_collection(\"gemini_rag\", embedding_function=embed_fn)\n",
        "    ids = [str(i) for i in range(len(chunks))]\n",
        "    collection.add(documents=chunks, ids=ids)\n",
        "    return collection, embed_fn\n",
        "\n",
        "# Gemini answer using retrieved chunks\n",
        "def query_gemini(question, collection, embed_fn):\n",
        "    embed_fn.document_mode = False\n",
        "    results = collection.query(query_texts=[question], n_results=3)\n",
        "    context = \"\\\\n\\\\n\".join(results[\"documents\"][0])\n",
        "\n",
        "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "    prompt = f\\\"\\\"\\\"You are a helpful assistant. Use the context below to answer the question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer:\\\"\\\"\\\"\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text, results[\"documents\"][0]\n",
        "\n",
        "# State for Gradio interface\n",
        "uploaded_chunks = []\n",
        "collection = None\n",
        "embed_fn = None\n",
        "\n",
        "# Upload and process PDF\n",
        "def handle_upload(file):\n",
        "    global uploaded_chunks, collection, embed_fn\n",
        "    uploaded_chunks = process_pdf(file.name)\n",
        "    collection, embed_fn = load_into_chroma(uploaded_chunks)\n",
        "    return f\"‚úÖ Indexed {len(uploaded_chunks)} text chunks.\"\n",
        "\n",
        "# Chat function\n",
        "def rag_chat(message, history):\n",
        "    if not collection:\n",
        "        return \"‚ö†Ô∏è Please upload a PDF first.\"\n",
        "    answer, sources = query_gemini(message, collection, embed_fn)\n",
        "    return answer\n",
        "\n",
        "# Gradio Interface\n",
        "with gr.Blocks() as app:\n",
        "    gr.Markdown(\"## üìÑ Gemini RAG PDF Chatbot\")\n",
        "    gr.Markdown(\"Upload a PDF and ask questions. Gemini 1.5 Flash will answer using the document.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        file_input = gr.File(label=\"üì§ Upload PDF\", file_types=[\".pdf\"])\n",
        "        file_output = gr.Textbox(label=\"Status\")\n",
        "\n",
        "    file_input.change(fn=handle_upload, inputs=file_input, outputs=file_output)\n",
        "\n",
        "    chatbot_ui = gr.ChatInterface(\n",
        "        fn=rag_chat,\n",
        "        title=\"ü§ñ Ask Your PDF\",\n",
        "        theme=\"soft\",\n",
        "        # retry_btn=\"üîÑ Retry\",\n",
        "        # clear_btn=\"üßπ Clear Chat\",\n",
        "    )\n",
        "    chatbot_ui.render()\n",
        "\n",
        "# Run the app\n",
        "if __name__ == \"__main__\":\n",
        "    app.launch(share=True)\n",
        "\n",
        "\"\"\"\n",
        "with open(\"chatbot_gradio_rag.py\", \"w\") as f:\n",
        "    f.write(code)\n",
        "\n",
        "import os\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"GENERATIVE_AI_API_KEY\"\n",
        "\n",
        "\n",
        "# !python chatbot_gradio_rag.py & npx localtunnel --port 7860\n",
        "!!python chatbot_gradio_rag.py\n",
        "\n"
      ],
      "metadata": {
        "id": "QhGSMOMrRqwf",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qQTGTmQOUO5x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}